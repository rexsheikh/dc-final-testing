\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{enumitem}

\geometry{margin=1in}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

\begin{document}

\begin{center}
{\LARGE \textbf{Text-to-Anki Flashcard Service Report}}\\[0.5em]
{\large Based on \texttt{ARCHITECTURE.md}}
\end{center}

\section{Software and Hardware Components}

The service is intentionally lightweight so that every tier can run on very basic Google Compute Engine VMs while keeping dependencies in the Python standard library plus a minimal set of supporting tools.

\begin{longtable}{p{0.28\linewidth} p{0.32\linewidth} p{0.32\linewidth}}
\toprule
\textbf{Component} & \textbf{Role} & \textbf{Notes / Configuration} \\
\midrule
Ubuntu 22.04 LTS (image family \texttt{ubuntu-2204-lts}) & Base OS image for all tiers & Provisioned once, snapshotted, and cloned for REST + worker VMs. \\
\addlinespace
Python 3.10 + standard library & Runtime for Flask API, worker, and NLP pipeline & Pure-Python FK, TF-IDF, and regex-based NER implementations avoid heavy ML frameworks. \\
\addlinespace
Flask 3.0 + Werkzeug & REST API tier & Handles upload/status/download endpoints. Deployed on a single VM tagged \texttt{allow-5000}. \\
\addlinespace
Redis 5.x & Queue + metadata store & LPUSH/BRPOP job queue plus job status hashes/key-value pairs. Initially local to REST VM; designed to migrate to Cloud Memorystore. \\
\addlinespace
NLP worker scripts (\texttt{worker.py}, \texttt{nlp.py}) & Transform text to decks & Workers consume queue jobs, run FK + lightweight pipeline, and emit CSV output. \\
\addlinespace
Deployment tooling (\texttt{setup\_base\_vm.sh}, \texttt{create\_rest\_tier.py}, \texttt{create\_workers.py}) & Provisioning + automation & Creates snapshots, firewall rules, and per-tier VMs with systemd-based startup scripts for robust process management. \\
\addlinespace
Verification scripts (\texttt{verify\_setup.sh}, \texttt{check\_queue\_process.sh}, \texttt{debug\_rest\_server.sh}) & Diagnostics and smoke tests & Validate gcloud config, VM state, systemd service status, environment variables, and REST endpoints after deployment. \\
\addlinespace
REST VM hardware & \texttt{e2-medium} (2 vCPU, 4 GB RAM) & Hosts Flask + Redis + shared disk mounted at \texttt{/mnt/shared}. \\
\addlinespace
Worker VMs & Two \texttt{f1-micro} instances (0.2 vCPU, 614 MB RAM) & Each pulls from Redis, processes text in 2--5 seconds, and writes to \texttt{/tmp/outputs}. \\
\addlinespace
Storage & Shared persistent disk (\texttt{/mnt/shared}) & REST uploads and worker reads both use \texttt{/mnt/shared/uploads} and \texttt{/mnt/shared/outputs}; later upgrade path is Cloud Storage. \\
\addlinespace
Networking & Firewall rule \texttt{allow-5000}, internal SSH & Public TCP/5000 for REST. Internal traffic between tiers over default VPC. \\
\bottomrule
\end{longtable}

\section{System Architecture and Interactions}

\subsection*{Architectural Diagram}
\begin{figure}[h]
\centering
\begin{verbatim}
 +---------+        +--------------------+        +------------------+
 |  Client |  HTTP  |  REST Tier (Flask) |  LPUSH |     Redis Queue  |
 | (upload)|------->|  /upload /status   |<------>|  + Job metadata   |
 +---------+        +--------------------+        +------------------+
      ^                    |       ^                       |
      |                    |       |                       |
      |      download      |       | BRPOP job             |
      |                    v       |                       v
 +---------+       +--------------------+        +------------------+
 |  Client |<------|  REST Tier (CSV)   |        |  Worker Pool     |
 |download |       +--------------------+        | (f1-micro VMs)   |
 +---------+                                       | read text, run |
                                                   | FK + NLP,      |
                                                   | write /tmp CSV |
                                                   +----------------+
\end{verbatim}
\caption{Logical architecture of the Text-to-Anki Service.}
\end{figure}

\subsection*{Interaction Narrative}
\begin{enumerate}[itemsep=0.3em]
    \item Clients upload \texttt{.txt} files to the Flask REST tier, which writes them to the shared disk under \texttt{/mnt/shared/uploads} and enqueues job IDs in Redis using \texttt{LPUSH}.
    \item Workers running on separate \texttt{f1-micro} VMs perform blocking pops (\texttt{BRPOP}) against the \texttt{job\_queue}, fetch the associated metadata, and read the text directly from the shared mount (no scp/HTTP hops needed).
    \item Each worker executes \texttt{nlp.py}: it calculates Flesch-Kincaid metrics, extracts keywords/entities, and emits complex-word CSV decks back onto \texttt{/mnt/shared/outputs} so the REST tier can stream them to clients.
    \item Job status documents inside Redis are updated to reflect \texttt{queued → processing → completed/failed}, enabling the REST tier to serve \texttt{/status/<job\_id>} queries.
    \item Clients download completed decks through \texttt{/download/<job\_id>}, which streams the CSV from local storage.
\end{enumerate}

\section{Debugging, Training, and Testing Process}

\textbf{Debugging workflow.}
\begin{itemize}[itemsep=0.2em]
    \item The setup script now performs an SSH readiness loop before copying dependency installers, preventing ``connection refused'' errors on brand-new instances.
    \item \texttt{verify\_setup.sh} and \texttt{check\_queue\_process.sh} assert that required gcloud services, firewall rules, VMs, and systemd services exist and are running before declaring successful deployment.
    \item \texttt{check\_queue\_process.sh} provides comprehensive diagnostics: systemd service status, metadata server values, environment variables from running processes, shared storage access, and recent logs from both journalctl and traditional log files.
    \item Systemd integration enables powerful debugging commands: \texttt{systemctl status anki-worker} shows service health, \texttt{journalctl -u anki-worker -f} streams live logs, and automatic restart-on-failure ensures workers recover from transient errors without manual intervention.
    \item Redis metadata allows replaying failed jobs by inspecting the stored file path and rerunning \texttt{worker.py} locally with the same payload.
\end{itemize}

\textbf{Training/testing mechanisms.}
\begin{itemize}[itemsep=0.2em]
    \item The lightweight NLP stack is deterministic and does not require model training; its heuristics (FK, TF-IDF, regex NER) are verified via unit-style scripts such as \texttt{test\_nlp.py} and \texttt{test\_pipeline.py}, along with manual CLI runs (`python nlp.py sample-data/...`).
    \item The README and deployment docs prescribe sample invocations that act as regression tests: running the pipeline against a fixed snippet, generating CSV output, and validating counts and keywords.
    \item Workers and the REST tier are smoke-tested with \texttt{quick-verify.sh} to ensure Redis connectivity, queue operations, and HTTP endpoints behave as expected after each deployment.
\end{itemize}

\section{System Behavior, Capacity, and Bottlenecks}

\textbf{Working system explanation.}
\begin{itemize}[itemsep=0.2em]
    \item Single REST VM (\texttt{e2-medium}) handles uploads, metadata queries, and CSV downloads while co-locating Redis for a simple deployment footprint. The REST tier runs as a systemd service (\texttt{anki-rest.service}) with auto-restart on failure and integrated logging via journalctl.
    \item Two worker VMs (\texttt{f1-micro}) pull jobs concurrently via \texttt{BRPOP}; each worker runs as a systemd service (\texttt{anki-worker.service}) ensuring automatic recovery from crashes with 10-second restart delays.
    \item Environment variables (\texttt{REDIS\_HOST}, \texttt{SHARED\_STORAGE\_ROOT}, \texttt{SHARED\_UPLOAD\_FOLDER}, \texttt{SHARED\_OUTPUT\_FOLDER}) are configured in systemd service files and also available via GCP instance metadata server, providing robust configuration with fallback mechanisms.
    \item Each worker job reads text from \texttt{/mnt/shared/uploads}, runs FK + TF-IDF + regex NER (2--5 seconds per document), generates a deck to \texttt{/mnt/shared/outputs}, and updates Redis status atomically.
    \item Shared storage at \texttt{/mnt/shared} enables workers to access uploaded files without network copies, while maintaining a clean upgrade path to Cloud Storage buckets with signed URLs.
\end{itemize}

\textbf{Supported workload.}
\begin{itemize}[itemsep=0.2em]
    \item With 2 workers at 2--5 seconds per job, throughput is roughly 20--30 text documents per minute, assuming typical lecture-sized inputs ($<$50 KB).
    \item The REST tier comfortably handles dozens of concurrent uploads/status requests; CPU utilization remains low because heavy work is offloaded to workers.
    \item Scaling simply means cloning additional worker VMs from the same snapshot and pointing them at the shared Redis queue.
\end{itemize}

\textbf{Potential bottlenecks.}
\begin{itemize}[itemsep=0.2em]
    \item Local Redis + filesystem make the REST VM a single point of failure; migrating to Cloud Memorystore and Cloud Storage would improve availability and horizontal scalability.
    \item The \texttt{f1-micro} instances have only 614 MB RAM; extremely large text files (multi-MB) could cause swapping or longer processing times. Input size limits (16 MB uploads, 50,000-character default processing window) mitigate this.
    \item Network egress from the REST VM could saturate if many large decks are downloaded simultaneously; in practice, CSVs are tiny ($\sim$KB), but heavier workloads would benefit from load-balanced REST instances.
    \item Systemd auto-restart (10-second delay) prevents cascading failures but can mask underlying configuration issues; \texttt{check\_queue\_process.sh} includes restart count monitoring to detect problematic crash loops.
\end{itemize}

\section{Summary}

The Autodeck service deliberately prioritizes predictable deployment and datacenter-pattern reinforcement (snapshots, queues, systemd process management, monitoring scripts, shared storage mounts) over sophisticated NLP. By running everything on small Google Compute Engine VMs with pure-Python tooling and systemd-managed services, the stack remains easy to debug, test, and reason about while demonstrating production-grade reliability patterns (auto-restart, integrated logging, environment isolation). Scaling paths (Cloud Memorystore, Cloud Storage, load-balanced REST tiers) are documented for future iterations once the coursework requirements are satisfied. 

\end{document}
